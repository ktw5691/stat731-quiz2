---
title: "STAT-731 Recitation 3"
author: "K. Tyler Wilcox"
date: "September 21, 2016"
output:
  beamer_presentation:
    highlight: zenburn
    incremental: false
    theme: "metropolis"
---

Preliminaries
========================================================

```{r, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

- Expected Value
- Moments
- Variance
- Law of Large Numbers
- Chebyshev's Theorem

Expected Value: Discrete Random Variables
========================================================

- Let $X$ be a discrete random variable
- Let $p_X(x)$ be the pmf of X with support in $\mathcal{X}$
- The expected value (theoretical average) of X:
$$\mathbb{E}[X] = \sum\limits_{x \in \mathcal{X}} x \cdot p_X(x)$$

***

- What would we roll on average from a six-sided fair die?

- $\mathcal{X} = \{ 1, 2, 3, 4, 5, 6 \}$
- $p_X(x) = \frac{1}{6}, x \in \mathcal{X}$

- $\mathbb{E}[X] = \sum\limits_{x \in \mathcal{X}} x \cdot p_X(x)$
- $\mathbb{E}[X] = (1)(\frac{1}{6}) + (2)(\frac{1}{6}) + (3)(\frac{1}{6}) + (4)(\frac{1}{6}) + (5)(\frac{1}{6}) + (6)(\frac{1}{6})$
- $\mathbb{E}[X] = 3.5$

```{r}
n = 10000L
x = 1L:6L
```

Expected Value: Continuous Random Variables
========================================================

- Let $X$ be a continuous random variable
- Let $p_X(x)$ be the pdf of X with support in $\mathcal{X}$
- The expected value (theoretical average) of X:
$$\mathbb{E}[X] = \int\limits_{x \in \mathcal{X}} x \cdot p_X(x) dx$$

***

- $X \in [0, \theta]$
- $p_X(x) = \theta ^ {-1}$
- $\mathbb{E}[X] = \int\limits_{x \in \mathcal{X}} x \cdot p_X(x) dx$
- $\mathbb{E}[X] = \int\limits_{0} ^ {\theta} x \cdot \theta ^ {-1} dx$
- $\mathbb{E}[X] = x ^ 2 \cdot (2 \theta) ^ {-1} \vert_0 ^ \theta$
- $\mathbb{E}[X] = \frac{\theta}{2}$

Variance
========================================================

- As useful as the first moment can be, we are often interested in the second moment
- Let $X$ be a continuous random variable
- Let $p_X(x)$ be the pdf of X with support in $\mathcal{X}$
- The second moment of X:
$$\mathbb{E}[X ^ 2] = \int\limits_{x \in \mathcal{X}} x ^ 2 \cdot p_X(x) dx$$
- Let $\mathbb{V}[X]$ be the variance of $X$:
$$\mathbb{V}[X] = \mathbb{E}[X ^ 2] - \mathbb{E}[X] ^ 2$$

***

- From the last example
- $X \in [0, \theta]$
- $p_X(x) = \theta ^ {-1}$
- $\mathbb{E}[X ^ 2] = \int\limits_{x \in \mathcal{X}} x ^ 2 \cdot p_X(x) dx$
- $\mathbb{E}[X ^ 2] = \int\limits_{0} ^ {\theta} x ^ 2 \cdot \theta ^ {-1} dx$
- $\mathbb{E}[X ^ 2] = x ^ 3 \cdot (3 \theta) ^ {-1} \vert_0 ^ \theta$
- $\mathbb{E}[X ^ 2] = \frac{\theta ^ 2}{3}$
- $\mathbb{V}[X] = \mathbb{E}[X ^ 2] - \mathbb{E}[X] ^ 2$
- $\mathbb{V}[X] = \frac{\theta ^ 2}{3} - \frac{\theta}{2} ^ 2$
- $\mathbb{V}[X] = \frac{\theta ^ 2}{12}$

Moments of Functions
========================================================

- We took the expectation of a function to obtain $\mathbb{V}[X]$
- More specifically
$\mathbb{V}[X] = \mathbb{E}[(X - \mathbb{E}[X]) ^ 2]$
- $\mathbb{V}[X] = \mathbb{E}[g(X)]$ where $g(X) = (X - \mathbb{E}[X]) ^ 2$
$$\mathbb{E}[g(X)] = \int\limits_{x \in \mathcal{X}} g(X) \cdot p_X(x) dx$$

Expectations with Constants
========================================================

- Let $a, b \in \mathbb{R}$
- $\mathbb{E}[aX + b] = \mathbb{E}[aX] + \mathbb{E}[b]$
- $\mathbb{E}[aX + b] = a \mathbb{E}[X] + b$

Wrapping Up
========================================================

- Probability Mass Functions
- Probability Density Functions
- Cumulative Distribution Functions
- Approximation with Monte Carlo
- Questions?
