---
title: "STAT-731 Recitation 4"
author: "K. Tyler Wilcox"
date: "September 28, 2016"
output:
  beamer_presentation:
    highlight: zenburn
    incremental: false
    theme: "metropolis"
---

```{r, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

Overview
========================================================

- Simulating Random Variables
- Special Discrete Distributions
- Chebyshev's Theorem
- Law of Large Numbers

Simulating Common Discrete Distributions
========================================================

- We've been doing this already
- Very useful way to deeply understand distributions
- Good way to simulate processes, models, and parameterizations

Example 1
========================================================

```{r, message = FALSE}
library(tidyverse); library(scales)
set.seed(55909666L)

r   = 5L
p   = 0.5
x   = 0L:(r + 10L)
px  = dnbinom(x, size = r, prob = p)
xpx = tibble(x = factor(x), px = px)
```

***

```{r, fig.height = 4, fig.cap = "$X \\sim \\text{NegBin}(n = 5, p = 0.5)$"}
xpx %>%
  ggplot(aes(x, px)) +
  geom_point() +
  theme_minimal()
```

Example 2
========================================================

```{r}
m   = 5L
p   = 1.0 / 20.0
x   = 0L:(m + 100L)
px  = dnbinom(x, size = m, prob = p)
xpx = tibble(x = factor(x), px = px)
```

***

```{r, echo = FALSE, fig.height = 6, fig.cap = "$X \\sim \\text{NegBin}(n = 5, \\theta = \\frac{1}{20})$"}
xpx %>%
  ggplot(aes(x, px)) +
  geom_point() +
  scale_x_discrete(breaks = seq(0L, 100L, by = 10L)) +
  theme_minimal()
```

Example 3
========================================================

```{r}
m      = 20L
lambda = 1.0
x      = 0L:m
px     = dpois(x, lambda)
xpx    = tibble(x = factor(x), px = px)
```

***

```{r, echo = FALSE, fig.height = 6, fig.cap = "$X \\sim \\text{Pois}(\\lambda = 1)$"}
xpx %>%
  ggplot(aes(x, px)) +
  geom_point() +
  theme_minimal()
```

Example 4
========================================================

```{r}
s   = 15L
f   = 10L
m   = 5L
x   = 0L:m
px  = dhyper(x, m = s, n = f, k = m)
xpx = tibble(x = factor(x), px = px)
```

***

```{r, echo = FALSE, fig.height = 6, fig.cap = "$X \\sim \\text{Hypergeo}(S = 15, F = 10, n = 5)$"}
xpx %>%
  ggplot(aes(x, px)) +
  geom_point() +
  theme_minimal()
```

Discrete Random Variables: Bernoulli Distribution
========================================================

- Consider a random variable $X$, $\mathcal{X} = \{ 0, 1 \}$ with $Pr[X = 1] = \theta$
- The probability mass function of $X$ is:
$$f_X(x) = \theta ^ {x} (1 - \theta) ^ {1 - x}, 0 < \theta  < 1$$
- We used this distribution when we considered a reasonable description of a coin flip

Simulating from the Bernoulli Distribution
========================================================

- Let $\theta = \frac{3}{4}$
- $\mathbb{E}[X] = \theta = \frac{3}{4}$
- Generate $m = 100$ empirical replications from Ber($\theta$)

```{r}
m     = 100L
n     = 1L
theta = 0.75
x     = rbinom(m, n, prob = theta)
avg   = mean(x)
```

- Empirical mean $\bar{X} = `r sprintf("%.4f", avg)`$

***

```{r, echo = FALSE, message = FALSE, fig.height = 4, fig.cap = "Empirical draws from $X \\sim \\text{Ber}(\\theta = 0.75)$"}
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 1.0), breaks = seq(0.0, 1.0, 0.1)) +
  theme_minimal()
```

***

- Let $\theta = \frac{1}{3}$
- $\mathbb{E}[X] = \theta = \frac{1}{3}$

```{r}
theta = 1.0 / 3.0
x     = rbinom(m, n, prob = theta)
avg   = mean(x)
```

***

- Empirical mean $\bar{X} = `r sprintf("%.4f", avg)`$

```{r, echo = FALSE, message = FALSE, fig.height = 6, fig.cap = "Empirical draws from $X \\sim \\text{Ber}(\\theta = \\frac{1}{3})$"}
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 1.0), breaks = seq(0.0, 1.0, 0.1)) +
  theme_minimal()
```

Discrete Random Variables: Binomial Distribution
========================================================

- Consider $n$ iid Bernoulli random variables $X_1, \ldots, X_n$
- Let $Y = \sum_{i = 1} ^ n X_i \sim \text{Binomial}(n, \theta)$
- The probability mass function of $Y$ is:
$$f_X(x) = {n \choose x} \theta ^ {x} (1 - \theta) ^ {n - x}, x \ge 0, \theta > 0$$
- $\mathbb{E}[X] = n \theta$
- Very common distribution with many applications

Simulating the Binomial as Sum of Bernoulli RVs
========================================================

- $\theta = 0.25$
- $\mathbb{E}[X] = n \theta = 1.25$

```{r}
m     = 1000L
n     = 5L
theta = 0.25
x = apply(matrix(rbinom(m * n, size = 1L,
                        prob = theta),
                 ncol = m), 2, sum)
xbar = mean(x)
```

- Empirical $\bar{X} = `r sprintf("%.4f", xbar)`$

***

```{r, echo = FALSE, fig.height = 4, fig.cap = "Empirical approximation of $X \\sim \\text{Bin}(n = 5, \\theta = 0.25)$ using draws from $X \\sim \\text{Ber}(\\theta = 0.25)$"}
x = tibble(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 1.0), breaks = seq(0.0, 1.0, 0.1)) +
  theme_minimal()
```

Simulating the Binomial Distribution Directly
========================================================

- Let $\theta = 0.25$
- Generate $m = 1000$ empirical replications from Bin($n = 5, \theta$)

```{r}
m     = 1000L
n     = 5L
theta = 0.25
x     = rbinom(m, n, prob = theta)
```

***

- Empirical $\bar{X} = `r sprintf("%.4f", mean(x))`$

```{r, echo = FALSE, fig.height = 5, fig.cap = "Empirical draws from $X \\sim \\text{Bin}(n = 5, \\theta = 0.25)$"}
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 1.0), breaks = seq(0.0, 1.0, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

Gaussian Approximation to Binomial Distribution
========================================================

- If $n$ is large, the Binomial distribution is relatively symmetric
- From the de Moivre-Laplace Theorem:
$$\text{Bin}\left( n, \theta \right) \stackrel{n}{\rightarrow} \text{N}\left(n \theta, n \theta (1 - \theta) \right)$$
- Basis of proportion hypothesis test using $\text{N}(0, 1)$

***

- $n = 5$
- Positively skewed

```{r, echo = FALSE, message = FALSE, fig.cap = "$X \\sim \\text{Bin}(n = 5, \\theta = \\frac{1}{3})$ PMF", fig.height = 4.5}
n     = 5L
theta = 1.0 / 3.0
x = rbinom(m, n, prob = theta)
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 0.5), breaks = seq(0.0, 0.5, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

***

- $n = 10$
- More symmetric, still skewed

```{r, echo = FALSE, message = FALSE, fig.cap = "$X \\sim \\text{Bin}(n = 10, \\theta = \\frac{1}{3})$ PMF", fig.height = 4.5}
n     = 10L
theta = 1.0 / 3.0
x = rbinom(m, n, prob = theta)
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 0.5), breaks = seq(0.0, 0.5, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

***

- $n = 30$
- Symmetric, approaching Gaussian distribution

```{r, echo = FALSE, message = FALSE, fig.cap = "$X \\sim \\text{Bin}(n = 30, \\theta = \\frac{1}{3})$ PMF", fig.height = 4.5}
n     = 30L
theta = 1.0 / 3.0
x = rbinom(m, n, prob = theta)
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 0.5), breaks = seq(0.0, 0.5, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

***

- $n = 100$
- Symmetric, approaching Gaussian distribution

```{r, echo = FALSE, message = FALSE, fig.cap = "$X \\sim \\text{Bin}(n = 100, \\theta = \\frac{1}{3})$ PMF", fig.height = 4.5}
n     = 100L
theta = 1.0 / 3.0
x = rbinom(m, n, prob = theta)
x = tibble(x = x) %>%
  mutate(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 0.5), breaks = seq(0.0, 0.5, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

Geometric Distribution
========================================================

- Let $X$ be the number of failures in a sequence of iid Bernoulli trials before a success occurs
- $\mathcal{X} = \{0, 1, ..., \infty\}$
- The geometric probability mass function:
$$f_X(x) = Pr[X = x] = \theta (1 - \theta) ^ x, x \ge 0, \theta > 0$$
- The geometric cumulative distribution function:
$$F_X(x) = 1 - (1 - \theta) ^ {k + 1}, x \ge 0, \theta > 0$$
- **Note:** in `R`, the geometric distribution `rgeom` has support for $X \in [0, \infty)$
- $\mathbb{E}[X] = \frac{1 - \theta}{\theta}$

***

- Consider $X \sim \text{Geom}(\theta)$ where $\theta = 0.4$
- $\mathbb{E}[X] = \frac{1 - \theta}{\theta} = 1.5$

```{r}
n     = 1000L
theta = 0.4
x     = rgeom(n, prob = theta)
expx  = (1.0 - theta) / theta
xbar  = mean(x)
```

- Empirical $\bar{X} = `r sprintf("%.4f", xbar)`$

***

```{r, echo = FALSE, fig.height = 4, fig.cap = "$X \\sim \\text{Geo}(\\theta = 0.4)$"}
x = tibble(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 1.0), breaks = seq(0.0, 1.0, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

***

- Consider $X \sim \text{Geom}(\theta)$ where $\theta = 0.8$
- $\mathbb{E}[X] = \frac{1 - \theta}{\theta} = 0.25$

```{r}
n     = 1000L
theta = 0.8
x     = rgeom(n, prob = theta)
expx  = (1.0 - theta) / theta
xbar  = mean(x)
```

- Empirical $\bar{X} = `r sprintf("%.4f", xbar)`$

***

```{r, echo = FALSE, fig.height = 4, fig.cap = "$X \\sim \\text{Geo}(\\theta = 0.8)$"}
x = tibble(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 1.0), breaks = seq(0.0, 1.0, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

Negative Binomial Distribution
========================================================

- Let $X$ be the number of failures in a sequence of iid Bernoulli trials before the $r$th success occurs
- $\mathcal{X} = \{0, 1, ..., \infty\}$
- The negative binomial probability mass function:
$$f_X(x) = Pr[X = x] = {{x + r - 1} \choose {x}} \theta ^ r (1 - \theta) ^ x, x \ge 0, \theta > 0$$
- $\mathbb{E}[X] =\frac{r (1 - \theta)}{\theta}$

Negative Binomial Distribution as Sum of IID Geometric Random Variables
========================================================

- Given $X_1, X_2, \ldots, X_n$ where $X_i \overset{iid}{\sim} F_X(x)$
- If $Y = \sum\limits_{i = 1}^{n} X_i$ then
$$M_Y(t) = \prod_{i = 1} ^ n M_{X_i}(t)$$

***

- Let $X_i \overset{iid}{\sim} \text{Geo}(\theta), i = 1, \ldots, r$
- $M_{X_i}(t) = \frac{\theta}{1 - (1 - \theta) e ^ t}$
\begin{align*}
  M_Y(t) &= \prod_{i = 1} ^ {r} M_{X_i}(t) \\
         &= \prod_{i = 1} ^ {r} \frac{\theta}{1 - (1 - \theta) e ^ t} \\
  M_Y(t) &= \left( \frac{\theta}{1 - (1 - \theta) e ^ t} \right) ^ r
\end{align*}
- $M_Y(t)$ can be recognized as the MGF of the Negative Binomial with $r$ failures

Simulating the Negative Binomial as Sum of Geometric RVs
========================================================

- $\theta = 0.5$
- $\mathbb{E}[X] = \frac{r ( 1 - \theta)}{\theta} = 5$

```{r}
m     = 1000L
r     = 5L
theta = 0.5
x = apply(matrix(rgeom(m * r,
                       prob = theta),
                 ncol = m), 2, sum)
xbar = mean(x)
```

- Empirical $\bar{X} = `r sprintf("%.4f", xbar)`$

***

```{r, echo = FALSE, fig.height = 4, fig.cap = "Empirical approximation of $X \\sim \\text{NegBin}(r = 5, \\theta = 0.5)$ using draws from $X \\sim \\text{Geo}(\\theta = 0.5)$"}
x = tibble(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 0.5), breaks = seq(0.0, 0.5, 0.1)) +
  theme_minimal()
```

***

- Using the negative binomial directly:

```{r}
n     = 1000L
r     = 5L
theta = 0.5
x     = rnbinom(n, size = r, prob = theta)
xbar  = mean(x)
```

- Empirical $\bar{X} = `r sprintf("%.4f", xbar)`$

***

```{r, echo = FALSE, fig.height = 4, fig.cap = "$X \\sim \\text{NegBin}(r = 5, \\theta = 0.5)$"}
x = tibble(x = factor(x))
x %>%
  ggplot() +
  geom_bar(aes(x, ..count.. / sum(..count..))) +
  scale_y_continuous(name = "Relative Frequency", labels = percent,
                     limits = c(0.0, 0.5), breaks = seq(0.0, 0.5, 0.1)) +
  ggtitle("Empirical Probability of X") +
  theme_minimal()
```

Arbitrary Discrete Distribution
========================================================

- Suppose $X \in {x_1, x_2, x_3, \ldots, x_k}$
- $P_X(x) \in {p(x_1), p(x_2), p(x_3), \ldots, p(x_k)}$
- Let $\mathcal{X} = {-2, -1, 0, 3, 4}$
- Let $P_X(x) = {0.125, 0.25, 0.25, 0.25, 0.125}$
- $\mathbb{E}[X] = 0.75$
- $\mathbb{E}[X ^ 2] = 5$
- $\mathbb{V}[X] = \mathbb{E}[X ^ 2] - \mathbb{E}[X] ^ 2 = 4.4375$

***

```{r}
xpx = tibble(
  x  = c(-2, -1, 0, 3, 4),
  px = c(0.125, 0.25, 0.25, 0.25, 0.125))
```

```{r, echo = FALSE, fig.cap = "An arbitrary PMF for $X \\sim F_X(x)$", fig.height = 4.5}
xpx %>%
  ggplot() +
  geom_point(aes(x, px)) +
  scale_x_continuous(breaks = -2L:4L, labels = -2L:4L) +
  scale_y_continuous(name = "Pr[X = x]", limits = c(0.0, 0.3),
                     breaks = seq(0.0, 0.3, 0.05)) +
  theme_minimal()
```

***

- Simulate $n$ random variates from $F_X(x)$

```{r}
n = 100000L
d = sample(xpx$x, size = n, replace = TRUE,
           prob = xpx$px)

# Empirical (sample) average/variance
xbar = mean(d)
varx = var(d)
```

- $\bar{X} = `r sprintf("%.4f", xbar)`$
- $s ^ 2 = `r sprintf("%.4f", varx)`$

Chebyshev's Inequality
========================================================

- Let $X$ be a random variable
- Let $g(X)$ be a non-negative function, i.e., $g(X) > 0$
- Then
$$Pr[g(X) \ge r] \le \frac{\mathbb{E}[g(X)]}{r}$$
- More commonly, we let $g(X) = \frac{(x - \mu) ^ 2}{\sigma ^ 2}$ and $r = t ^ 2$
\begin{align*}
  Pr[g(X) \ge r]                                              &\le
    \frac{\mathbb{E}[g(X)]}{r} \\
  Pr\left[ \frac{(x - \mu) ^ 2}{\sigma ^ 2} \ge t ^ 2 \right] &\le
    \frac{1}{t ^ 2} \mathbb{E}\left[ \frac{(x - \mu) ^ 2}{\sigma ^ 2} \right] =
    \frac{1}{t ^ 2} \\
  Pr\left[ |X - \mu| \le t \sigma \right] &\le \frac{1}{t ^ 2} \\
  Pr\left[ |X - \mu| < t \sigma \right] &\ge 1 - \frac{1}{t ^ 2}
\end{align*}

Example: Chebyshev's Inequality
========================================================

- We can now bound the distance between $X$ and $\mu$, $|X - \mu|$ in terms of $\sigma$ regardless of the distribution of $X$
- Let $t = 2$
- $Pr\left[ |X - \mu| < t \sigma \right] \ge 1 - \frac{1}{t ^ 2}$
- $Pr\left[ |X - \mu| < 2 \sigma \right] \ge 1 - \frac{1}{4}$
- $Pr\left[ |X - \mu| < 2 \sigma \right] \ge 0.75$
- Random variable $X$ has at least a 75% chance of being within 2 standard
deviations of its mean
- Unfortunately, it's very conservative (sometimes not helpful)
- What is the bound if $t = 1$? $t = 0.5$?

Poisson Distribution
========================================================

- Let $X$ be the number of events in an interval
- $\mathcal{X} = \{0, 1, ..., \infty\}$
- The Poisson probability mass function:
$$f_X(x) = Pr[X = x] = \frac{\lambda ^ k \exp\{ -\lambda \}}{k!}, x \ge 0, \lambda > 0$$
- $\mathbb{E}[X] = \lambda$
- $\mathbb{V}[X] = \lambda$

Chebyshev's Inequality for Poisson Distribution
========================================================

```{r}
n      = 10000L
lambda = 1.0
t      = 2.0
x      = rpois(n, lambda)
prob   = length(which(
  abs(x - lambda) < t * sqrt(lambda))) / n
```

- Chebyshev's Inequality: $Pr\left[ |X - 1| < 2 \right] \ge 0.75$
- Empirically, $Pr\left[ |X - 1| < 2 \right] \dot{=} `r sprintf("%.4f", prob)`$
- Note that the empirical probability of $X$ being within 2 standard deviations of the mean when $X \sim \text{Pois}(\lambda = 1)$ is greater than the lower bound given by Chebyshev's Inequality

Chebyshev's Inequality for Geometric Distribution
========================================================

```{r}
n  = 10000L
p  = 0.25
t  = 2.0
ex = (1.0 - p) / p
vx = (1.0 - p) / (p * p)
x  = rgeom(n, p)
prob = length(which(
  abs(x - ex) < t * sqrt(vx))) / n
```

- Chebyshev's Inequality: $Pr\left[ |X - 3| < 6.93 \right] \ge 0.75$
- Empirically, $Pr\left[ |X - 3| < 6.93 \right] \dot{=} `r sprintf("%.4f", prob)`$

Chebyshev's Inequality for Normal Distribution
========================================================

```{r}
n     = 10000L
ex    = 9.0
sigma = 2.0
vx    = sigma * sigma
t     = 1.96
x     = rnorm(n, mean = ex, sd = sigma)
prob  = length(which(
  abs(x - ex) < t * sigma)) / n
```

- Chebyshev's Inequality: $Pr\left[ |X - 9| < 3.92 \right] \ge 0.7397$
- Empirically, $Pr\left[ |X - 9| < 3.92 \right] \dot{=} `r sprintf("%.4f", prob)`$
- Clearly, for certain distributions, we can find better probabilistic bounds

Weak Law of Large Numbers
========================================================

- Let $X_1, X_2, \ldots, X_n$ be iid random variables
- Let $\mathbb{E}[X_i] = \mu$
- $\mathbb{V}[X_i] = \sigma ^ 2 < \infty$
- Define $\bar{X}_n = \frac{1}{n}\sum\limits_{i = 1} ^ n X_i$
- Then $\forall \epsilon > 0$
$$lim_{n \rightarrow \infty} Pr[|\bar{X}_n - \mu| < \epsilon] = 1$$
- $\bar{X}_n$ converges *in probability* to $\mu$
- We can also show that the sample variance $S^2_n$ converges in probability to $\sigma ^ 2$ (Quiz Question)

Strong Law of Large Numbers
========================================================

- Let $X_1, X_2, \ldots, X_n$ be iid random variables
- Let $\mathbb{E}[X_i] = \mu$
- $\mathbb{V}[X_i] = \sigma ^ 2 < \infty$
- Define $\bar{X}_n = \frac{1}{n}\sum\limits_{i = 1} ^ n X_i$
- Then $\forall \epsilon > 0$
$$Pr[lim_{n \rightarrow \infty} |\bar{X}_n - \mu| < \epsilon] = 1$$
- $\bar{X}_n$ converges almost surely to $\mu$

LLN Example
========================================================

- Draw $n$ random variates from $X \sim \text{Pois}(\lambda = 3)$

```{r}
m       = 100L
epsilon = 0.01
vn      = c(100L, 500L, 1600L, 3200L,
            6400L, 9600L, 18000L,
            25600L, 54000L, 108000L,
            256000L, 819200L)
nn      = length(vn)
pgood   = numeric(nn)
mu      = lambda = 3.0
```

***

```{r}
for (j in 1L:nn) {
  n  = vn[j]
  xx = matrix(rpois(n * m, mu),
              ncol = n)
  xbar = apply(xx, 1L, mean)
  good = which(abs(xbar - mu) < epsilon)
  pgood[j] = length(good) / m
}
```

***

```{r, fig.cap = "Empirical Demonstration of the Law of Large Numbers for $X \\sim \\text{Pois}(\\lambda = 3)$", fig.height = 5}
plot(vn, pgood, type = 'b', xlab = 'n',
     ylab = 'Prob[|Xbar - mu| < epsilon]')
```

Wrapping Up
========================================================

- Simulating Random Variables
- Special Discrete Distributions
- Chebyshev's Theorem
- Law of Large Numbers
- Questions?
